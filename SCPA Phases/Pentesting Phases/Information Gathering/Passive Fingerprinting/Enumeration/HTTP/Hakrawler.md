# Hakrawler

## Setup

```
$ go install github.com/hakluke/hakrawler@latest && \
sudo cp go/bin/hakrawler /usr/local/bin
```

## Help Menu

```
$ hakrawler -h
flag needs an argument: -h
Usage of hakrawler:
  -d int
        Depth to crawl. (default 2)
  -dr
        Disable following HTTP redirects.
  -h string
        Custom headers separated by two semi-colons. E.g. -h "Cookie: foo=bar;;Referer: http://example.com/" 
  -i    Only crawl inside path
  -insecure
        Disable TLS verification.
  -json
        Output as JSON.
  -proxy string
        Proxy URL. E.g. -proxy http://127.0.0.1:8080
  -s    Show the source of URL based on where it was found. E.g. href, form, script, etc.
  -size int
        Page size limit, in KB. (default -1)
  -subs
        Include subdomains for crawling.
  -t int
        Number of threads to utilise. (default 8)
  -timeout int
        Maximum time to crawl each URL from stdin, in seconds. (default -1)
  -u    Show only unique urls.
  -w    Show at which link the URL is found.
```
## Usage

TODO: Provide more usage coverage for Hakrawler

`$ cat http[s]://<IP> | hakrawler -u`

`$ cat urls.txt | hakrawler -subs -u -d 4`
## References

- [hakrawler](https://github.com/hakluke/hakrawler)