# Gathering URLs

## 1. Manual

**Note:** some websites I've discovered that they couldn't work with `curl` or `wget` in this case go to **View Page Source** on any web browser of your choice and save it as html file then use the `grep` command to parse it.

`$ curl http[s]://<IP> | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" | cut -d "/" -f 3 | sort -u > urls.txt`

`$ wget -qO- http[s]://<IP> | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" | awk -F "/" '{print $3}' | sort -u > urls.txt`

`$ grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" file.html | awk -F "/" '{print $3}' | sort -u > urls.txt`

`$ curl http[s]://<IP>/file.js | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" | cut -d "/" -f 3 | sort -u > urls.txt`

`$ curl http[s]://<IP>/file.js | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" | awk -F "/" '{print $3}' | sort -u > urls.txt`

## 2. DataSurgeon

`$ curl http[s]://<IP> | ds -uC | uniq`

`$ wget -qO- http[s]://<IP> | ds -uC | uniq`